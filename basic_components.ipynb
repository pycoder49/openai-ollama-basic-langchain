{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "649c33f0",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "True"
      ]
     },
     "execution_count": 13,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import os\n",
    "from dotenv import load_dotenv\n",
    "load_dotenv()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "2bbc473f",
   "metadata": {},
   "outputs": [],
   "source": [
    "from langchain_openai import ChatOpenAI\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "0971793b",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "profile={'max_input_tokens': 272000, 'max_output_tokens': 128000, 'image_inputs': True, 'audio_inputs': False, 'video_inputs': False, 'image_outputs': False, 'audio_outputs': False, 'video_outputs': False, 'reasoning_output': True, 'tool_calling': True, 'structured_output': True, 'image_url_inputs': True, 'pdf_inputs': True, 'pdf_tool_message': True, 'image_tool_message': True, 'tool_choice': True} client=<openai.resources.chat.completions.completions.Completions object at 0x000001FF6B2EF9D0> async_client=<openai.resources.chat.completions.completions.AsyncCompletions object at 0x000001FF6C9882D0> root_client=<openai.OpenAI object at 0x000001FF6B2EE990> root_async_client=<openai.AsyncOpenAI object at 0x000001FF6C988050> model_name='gpt-5' model_kwargs={} openai_api_key=SecretStr('**********') stream_usage=True\n"
     ]
    }
   ],
   "source": [
    "llm = ChatOpenAI(\n",
    "    api_key=os.getenv(\"OPENAI_API_KEY\"),    \n",
    "    model=\"gpt-5\"\n",
    ")       \n",
    "# you can skip the api key sinceit's read from the env variables\n",
    "\n",
    "print(llm)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "1049f7a9",
   "metadata": {},
   "outputs": [],
   "source": [
    "answer = llm.invoke(\"What is LangChain?\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "7bc2794f",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "AIMessage(content='LangChain is an open‑source framework (Python and TypeScript) for building applications powered by large language models. It provides standard building blocks and integrations so you can compose model calls, tools, and data sources into reliable workflows.\\n\\nKey ideas and components:\\n- Model interfaces: unified wrappers for chat/LLMs, embeddings, prompt templating, and output parsing.\\n- Chains: sequences or graphs of steps you can wire together; the LangChain Expression Language (LCEL) lets you declaratively compose pipelines.\\n- Retrieval (RAG): document loaders, text splitters, embeddings, vector stores, and retrievers for grounding model outputs in your data.\\n- Agents and tools: tool-calling agents that decide which tools/APIs to use, with built‑in tool and function-calling support.\\n- Memory/state: patterns for chat history and stateful workflows.\\n- Ecosystem: \\n  - LangGraph for stateful, multi‑step/agent workflows as graphs.\\n  - LangServe for turning chains into deployed APIs.\\n  - LangSmith (hosted) for tracing, evaluation, and monitoring.\\n\\nCommon use cases:\\n- Retrieval‑augmented chat/search over your documents\\n- Tool‑using agents (e.g., call databases, web search, code execution)\\n- Data extraction/ETL and structured output generation\\n- Code assistants and task automation\\n\\nMinimal example (Python) of a simple chain:\\nfrom langchain_openai import ChatOpenAI\\nfrom langchain_core.prompts import ChatPromptTemplate\\nfrom langchain_core.output_parsers import StrOutputParser\\n\\nprompt = ChatPromptTemplate.from_messages([\\n    (\"system\", \"You are a concise assistant.\"),\\n    (\"user\", \"{question}\")\\n])\\nchain = prompt | ChatOpenAI(model=\"gpt-4o-mini\") | StrOutputParser()\\nresult = chain.invoke({\"question\": \"Explain RAG in one sentence.\"})\\n\\nWhen to use it:\\n- Helpful if you want rapid prototyping, lots of connectors, structured output, and observability.\\n- If you only need a single API call with minimal dependencies, calling a model SDK directly may be simpler.\\n\\nAlternatives include LlamaIndex (data/RAG centric), Haystack, Semantic Kernel, and Guidance; LangChain focuses on composability, tool use, and a broad integration layer.', additional_kwargs={'refusal': None}, response_metadata={'token_usage': {'completion_tokens': 1114, 'prompt_tokens': 11, 'total_tokens': 1125, 'completion_tokens_details': {'accepted_prediction_tokens': 0, 'audio_tokens': 0, 'reasoning_tokens': 640, 'rejected_prediction_tokens': 0}, 'prompt_tokens_details': {'audio_tokens': 0, 'cached_tokens': 0}}, 'model_provider': 'openai', 'model_name': 'gpt-5-2025-08-07', 'system_fingerprint': None, 'id': 'chatcmpl-Cq8rigoDctJDHBnE8xvKrErsjEiPC', 'service_tier': 'default', 'finish_reason': 'stop', 'logprobs': None}, id='lc_run--019b4e2d-986b-70f2-8936-157e78624937-0', usage_metadata={'input_tokens': 11, 'output_tokens': 1114, 'total_tokens': 1125, 'input_token_details': {'audio': 0, 'cache_read': 0}, 'output_token_details': {'audio': 0, 'reasoning': 640}})"
      ]
     },
     "execution_count": 17,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "answer"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0cd23ef0",
   "metadata": {},
   "source": [
    "### Prompt templating"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "9c0a2bde",
   "metadata": {},
   "outputs": [],
   "source": [
    "from langchain_core.prompts import ChatPromptTemplate"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "21c7d16e",
   "metadata": {},
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "id": "bfff4fe4",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "ChatPromptTemplate(input_variables=['input'], input_types={}, partial_variables={}, messages=[SystemMessagePromptTemplate(prompt=PromptTemplate(input_variables=[], input_types={}, partial_variables={}, template='You are a helpful assistant that helps people find information.'), additional_kwargs={}), HumanMessagePromptTemplate(prompt=PromptTemplate(input_variables=['input'], input_types={}, partial_variables={}, template='{input}'), additional_kwargs={})])"
      ]
     },
     "execution_count": 19,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "prompt_template = ChatPromptTemplate.from_messages([\n",
    "    (\"system\", \"You are a helpful assistant that helps people find information.\"),\n",
    "    (\"user\", \"{input}\")\n",
    "])\n",
    "prompt_template"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "id": "63333f83",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "AIMessage(content='LangChain is a toolkit that helps developers build apps powered by large language models (like ChatGPT). Think of it as the “plumbing” that connects a model to your data, tools, and a step-by-step plan so the model can actually get useful work done.\\n\\nKey ideas in plain terms:\\n- Prompt help: Reuses good prompts with variables so you don’t copy‑paste or make mistakes.\\n- Chains: Set up a sequence of steps (e.g., “summarize this email, then draft a reply, then format it”).\\n- Agents: Let the model decide which tool to use next (like “search the web,” “look in a database,” “use a calculator”).\\n- Tools and data: Plug in search, spreadsheets, code execution, APIs, or your own documents.\\n- Retrieval (RAG): Pull the right chunks from your files or knowledge base so the model answers with your data, not just what it was trained on.\\n- Memory: Keep track of what was said earlier in a conversation so the app stays consistent.\\n- Output parsing: Turn the model’s text into structured data (lists, JSON) that your app can use.\\n- Tracing/eval: See what happened step‑by‑step and test quality so you can debug and improve.\\n\\nSimple example:\\n- You want a chatbot that answers questions about your company handbook.\\n  1) Load and split the handbook into chunks.\\n  2) Index those chunks for fast lookup.\\n  3) When a user asks a question, retrieve the most relevant chunks.\\n  4) Feed the chunks plus the question to the model to produce an answer.\\n  LangChain provides the pieces to do each step and wire them together.\\n\\nWhen to use it:\\n- You need more than a single “ask the model once” call.\\n- You want retrieval over your documents, tool use, multi-step workflows, or agents.\\n- You care about testing, observability, and maintaining prompts at scale.\\n\\nWhen you might not need it:\\n- You just need one simple model call; a basic SDK may be enough.\\n\\nRough alternatives:\\n- Roll your own with an OpenAI/Anthropic SDK.\\n- LlamaIndex for document retrieval workflows.\\n- Lightweight orchestration or function-calling directly.\\n\\nIn short: LangChain is the glue that helps an LLM reliably use your data and tools through clear steps, making it much easier to build real applications.', additional_kwargs={'refusal': None}, response_metadata={'token_usage': {'completion_tokens': 1144, 'prompt_tokens': 28, 'total_tokens': 1172, 'completion_tokens_details': {'accepted_prediction_tokens': 0, 'audio_tokens': 0, 'reasoning_tokens': 640, 'rejected_prediction_tokens': 0}, 'prompt_tokens_details': {'audio_tokens': 0, 'cached_tokens': 0}}, 'model_provider': 'openai', 'model_name': 'gpt-5-2025-08-07', 'system_fingerprint': None, 'id': 'chatcmpl-Cq8s26iF15UAXnjKmRpNhJN2dYffs', 'service_tier': 'default', 'finish_reason': 'stop', 'logprobs': None}, id='lc_run--019b4e2d-e8bb-7501-8ecd-b63b0f988e18-0', usage_metadata={'input_tokens': 28, 'output_tokens': 1144, 'total_tokens': 1172, 'input_token_details': {'audio': 0, 'cache_read': 0}, 'output_token_details': {'audio': 0, 'reasoning': 640}})"
      ]
     },
     "execution_count": 20,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "chain = prompt_template | llm   # acts like a pipeline\n",
    "\n",
    "response = chain.invoke({\"input\": \"Explain LangChain in simple terms.\"})\n",
    "response"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "id": "fcb62bda",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "langchain_core.messages.ai.AIMessage"
      ]
     },
     "execution_count": 21,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "type(response)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8722af9b",
   "metadata": {},
   "source": [
    "### String output parser"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "id": "da639730",
   "metadata": {},
   "outputs": [],
   "source": [
    "from langchain_core.output_parsers import StrOutputParser"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "id": "224a7f94",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'LangChain is a toolkit for building apps with large language models (LLMs). Think of it as Lego blocks plus the wiring that lets an LLM talk to your data, call tools, and follow multi-step workflows.\\n\\nWhat it does\\n- Connects to many models (OpenAI, Anthropic, local models) and embeddings with a common interface.\\n- Lets you chain steps: build a pipeline like “prepare prompt → call model → parse the answer → maybe call an API → continue.”\\n- Adds “memory” so a chatbot can remember context across turns.\\n- Makes retrieval easy (RAG): pull relevant passages from your documents or databases and give them to the model.\\n- Provides “agents” that can decide which tool to use next (search, code execution, calculators, custom APIs).\\n- Offers tracing/observability so you can see what happened inside a run.\\n- Works in Python and JavaScript/TypeScript.\\n\\nA simple mental model\\n- Components: prompts, models, output parsers, retrievers, tools.\\n- Chains: glue those components into a repeatable workflow.\\n- Agents: let the model choose which tool/step to run next.\\n- Data connectors: talk to files, vector databases, SQL, web APIs.\\n\\nTiny example use case\\n- You want a support bot that answers from your PDFs.\\n  1) Split PDFs into chunks and create embeddings.\\n  2) Store them in a vector database.\\n  3) On a question, retrieve the most relevant chunks.\\n  4) Put those chunks plus the question into a prompt and call the LLM.\\n  LangChain provides each piece and the wiring between them.\\n\\nWhen to use it\\n- Use LangChain if you need multi-step LLM logic, tool use, retrieval over your data, or you want to switch between model providers easily.\\n- If you only need a single prompt → single response, you might not need a framework at all.\\n\\nIn short: LangChain doesn’t make models; it helps you reliably connect models to data and tools and turn that into a real application.'"
      ]
     },
     "execution_count": 24,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "output_parser = StrOutputParser()\n",
    "\n",
    "chain = prompt_template | llm | output_parser\n",
    "\n",
    "response = chain.invoke({\"input\": \"Explain LangChain in simple terms.\"})\n",
    "response"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0196255f",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3c6dea77",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2fbcaaa5",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.13.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
